{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "175b7ef7-059f-447e-bcdf-57d4b704afda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\programdata\\anaconda3\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (2.3.4)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.16.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efaa49cf-f40f-48b3-8e39-231f07607f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Œ Loading dataset...\n",
      "Dataset Shape: (920, 16)\n",
      "\n",
      "ðŸ“Œ Converting target column (num)...\n",
      "Target Value Counts:\n",
      " num\n",
      "1    509\n",
      "0    411\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ðŸ“Œ Handling missing values...\n",
      "âœ… Missing values handled successfully!\n",
      "\n",
      "ðŸ“Œ Encoding categorical columns...\n",
      "Dataset Shape After Encoding: (920, 23)\n",
      "Training Set Shape: (736, 22)\n",
      "Testing Set Shape : (184, 22)\n",
      "\n",
      "ðŸ“Œ Training models and calculating metrics...\n",
      "\n",
      "Training: Logistic Regression\n",
      "Training: Decision Tree\n",
      "Training: KNN\n",
      "Training: Naive Bayes\n",
      "Training: Random Forest\n",
      "Training: XGBoost\n",
      "\n",
      "============================================\n",
      "âœ… FINAL MODEL COMPARISON TABLE\n",
      "============================================\n",
      "\n",
      "                 Model  Accuracy       AUC  Precision    Recall  F1 Score  \\\n",
      "0  Logistic Regression  0.847826  0.916820   0.878505  0.862385  0.870370   \n",
      "1        Decision Tree  0.782609  0.789480   0.863158  0.752294  0.803922   \n",
      "2                  KNN  0.809783  0.909297   0.877551  0.788991  0.830918   \n",
      "3          Naive Bayes  0.836957  0.909602   0.883495  0.834862  0.858491   \n",
      "4        Random Forest  0.891304  0.946177   0.915888  0.899083  0.907407   \n",
      "5              XGBoost  0.853261  0.938471   0.901961  0.844037  0.872038   \n",
      "\n",
      "        MCC  \n",
      "0  0.686371  \n",
      "1  0.569293  \n",
      "2  0.619478  \n",
      "3  0.668033  \n",
      "4  0.776051  \n",
      "5  0.702628  \n",
      "\n",
      "ðŸ“Œ Results saved as: model_comparison_results.csv\n",
      "\n",
      "ðŸŽ‰ Training Completed Successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Machine Learning Assignment 2\n",
    "# Heart Disease Classification - Train Models\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    matthews_corrcoef\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Step 1: Load Dataset\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nðŸ“Œ Loading dataset...\")\n",
    "\n",
    "df = pd.read_csv(\"heart_disease_uci.csv\")\n",
    "\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Step 2: Convert Target Column (num â†’ binary)\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nðŸ“Œ Converting target column (num)...\")\n",
    "\n",
    "# 0 â†’ No disease\n",
    "# 1,2,3,4 â†’ Disease present\n",
    "df[\"num\"] = df[\"num\"].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "print(\"Target Value Counts:\\n\", df[\"num\"].value_counts())\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Step 3: Handle Missing Values (NaNs)\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nðŸ“Œ Handling missing values...\")\n",
    "\n",
    "# Separate categorical and numeric columns\n",
    "categorical_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "numeric_cols = df.select_dtypes(exclude=[\"object\"]).columns\n",
    "\n",
    "# Fill numeric missing values with mean\n",
    "num_imputer = SimpleImputer(strategy=\"mean\")\n",
    "df[numeric_cols] = num_imputer.fit_transform(df[numeric_cols])\n",
    "\n",
    "# Fill categorical missing values with most frequent value\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])\n",
    "\n",
    "print(\"âœ… Missing values handled successfully!\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Step 4: Encode Categorical Columns\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nðŸ“Œ Encoding categorical columns...\")\n",
    "\n",
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "print(\"Dataset Shape After Encoding:\", df.shape)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Step 5: Split Features and Target\n",
    "# ============================================\n",
    "\n",
    "X = df.drop(\"num\", axis=1)\n",
    "y = df[\"num\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training Set Shape:\", X_train.shape)\n",
    "print(\"Testing Set Shape :\", X_test.shape)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Step 6: Define Models (All 6 Required)\n",
    "# ============================================\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=5000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=200),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric=\"logloss\")\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Step 7: Train Models + Evaluate Metrics\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nðŸ“Œ Training models and calculating metrics...\\n\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(\"Training:\", name)\n",
    "\n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Probabilities for AUC\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "    results.append([name, acc, auc, prec, rec, f1, mcc])\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Step 8: Comparison Table Output\n",
    "# ============================================\n",
    "\n",
    "comparison_df = pd.DataFrame(\n",
    "    results,\n",
    "    columns=[\"Model\", \"Accuracy\", \"AUC\", \"Precision\", \"Recall\", \"F1 Score\", \"MCC\"]\n",
    ")\n",
    "\n",
    "print(\"\\n============================================\")\n",
    "print(\"âœ… FINAL MODEL COMPARISON TABLE\")\n",
    "print(\"============================================\\n\")\n",
    "\n",
    "print(comparison_df)\n",
    "\n",
    "# Save results to CSV (optional)\n",
    "comparison_df.to_csv(\"model_comparison_results.csv\", index=False)\n",
    "\n",
    "print(\"\\nðŸ“Œ Results saved as: model_comparison_results.csv\")\n",
    "print(\"\\nðŸŽ‰ Training Completed Successfully!\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378a03b5-90e0-45fe-9060-4cab095ad4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
